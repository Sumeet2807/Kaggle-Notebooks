{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom sklearn.manifold import TSNE\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)\nfrom sklearn.preprocessing import OneHotEncoder as OHE\nfrom sklearn.metrics import r2_score\nimport random\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-06T18:23:42.633633Z","iopub.execute_input":"2022-11-06T18:23:42.634440Z","iopub.status.idle":"2022-11-06T18:23:51.629976Z","shell.execute_reply.started":"2022-11-06T18:23:42.634268Z","shell.execute_reply":"2022-11-06T18:23:51.628207Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/myanimelist-dataset/user_anime000000000032.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000000.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000029.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000019.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000011.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000025.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000040.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000047.csv\n/kaggle/input/myanimelist-dataset/user.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000002.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000009.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000033.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000006.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000068.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000034.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000061.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000012.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000001.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000053.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000028.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000044.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000014.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000048.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000036.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000057.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000004.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000024.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000005.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000069.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000003.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000027.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000010.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000060.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000055.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000054.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000050.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000016.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000042.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000017.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000052.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000065.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000008.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000035.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000063.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000046.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000015.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000062.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000064.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000037.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000030.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000039.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000058.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000056.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000066.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000026.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000043.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000049.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000022.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000038.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000013.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000051.csv\n/kaggle/input/myanimelist-dataset/anime.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000023.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000031.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000018.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000067.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000045.csv\n/kaggle/input/myanimelist-dataset/user_user.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000007.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000020.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000021.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000059.csv\n/kaggle/input/myanimelist-dataset/user_anime000000000041.csv\n/kaggle/input/myanimelist-dataset/anime_anime.csv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.14.0.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}}]},{"cell_type":"markdown","source":"****Collaborative filtering****\n\nconsiderations -  \n- extremely high number of users","metadata":{}},{"cell_type":"code","source":"anime  = pd.read_csv('../input/myanimelist-dataset/anime.csv', sep='\\t')\nanime_anime = pd.read_csv('../input/myanimelist-dataset/anime_anime.csv', sep='\\t')\nuser = pd.read_csv('../input/myanimelist-dataset/user.csv', sep='\\t')\nuser_user = pd.read_csv('../input/myanimelist-dataset/user_user.csv', sep='\\t')\nuser['serial_index'] = list(user.index)\nuser = user.set_index('user_id')\nanime['serial_index'] = list(anime.index)\nanime = anime.set_index('anime_id')","metadata":{"execution":{"iopub.status.busy":"2022-11-06T18:24:45.529872Z","iopub.execute_input":"2022-11-06T18:24:45.530425Z","iopub.status.idle":"2022-11-06T18:24:58.866722Z","shell.execute_reply.started":"2022-11-06T18:24:45.530381Z","shell.execute_reply":"2022-11-06T18:24:58.865373Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"files_to_skip = ['user.csv','anime.csv','user_user.csv','anime_anime.csv']\nfiles = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename in files_to_skip:\n            continue\n        files.append(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-11-06T18:24:58.868804Z","iopub.execute_input":"2022-11-06T18:24:58.869326Z","iopub.status.idle":"2022-11-06T18:24:58.879645Z","shell.execute_reply.started":"2022-11-06T18:24:58.869286Z","shell.execute_reply":"2022-11-06T18:24:58.877828Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# df = None\n# for file in files:  \n#     print(file)\n#     df_temp = pd.read_csv(file,sep='\\t')[['user_id', 'anime_id', 'score','last_interaction_date']]\n#     df_temp = df_temp[~df_temp['score'].isnull()]\n#     if df is None:\n#         df = df_temp\n#     else:\n#         df = pd.concat([df,df_temp])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#user anime interaction loader\nclass User_anime_data:\n    def __init__(self,file_list,user_ids,anime_ids,batch_size=32,shuffle=True):\n        self.user_ids = user_ids\n        self.anime_ids = anime_ids\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.file_list = file_list\n        \n    def get_gen(self):\n        if self.shuffle:\n            random.shuffle(self.file_list)\n        for file in self.file_list:\n            if isinstance(file, bytes):\n                file = file.decode('utf-8')\n            print(file)\n            df_file = pd.read_csv(str(file), sep='\\t')[['user_id','anime_id','score']]\n            df_file = df_file[~df_file['score'].isnull()]\n            print(len(df_file))\n            if self.shuffle:\n                df_file = df_file.sample(frac=1)\n            prev_i = 0\n            for i in range(self.batch_size,len(df_file),self.batch_size):\n                df_minibatch = df_file[prev_i:i]\n                users_minibatch = np.array(self.user_ids.loc[df_minibatch['user_id'].tolist()])\n                anime_minibatch = np.array(self.anime_ids.loc[df_minibatch['anime_id'].tolist()])\n                y = np.array(df_minibatch['score'])\n                X = np.stack((users_minibatch,anime_minibatch),axis=1)\n\n                yield X,y\n        \n        \n# def user_anime_data(file_list,user_ids,anime_ids,batch_size=32,shuffle=True):\n#     if shuffle:\n#         random.shuffle(file_list)\n#     for file in file_list:\n#         if isinstance(file, bytes):\n#             file = file.decode('utf-8')\n#         print(file)\n#         df_file = pd.read_csv(str(file), sep='\\t')[['user_id','anime_id','score']]\n#         df_file = df_file[~df_file['score'].isnull()]\n#         if shuffle:\n#             df_file = df_file.sample(frac=1)\n#         prev_i = 0\n#         for i in range(batch_size,len(df_file),batch_size):\n#             df_minibatch = df_file[prev_i:i]\n#             users_minibatch = np.array(user_ids.loc[df_minibatch['user_id'].tolist()])\n#             anime_minibatch = np.array(anime_ids.loc[df_minibatch['anime_id'].tolist()])\n#             y = np.array(df_minibatch['score'])\n#             X = np.stack((users_minibatch,anime_minibatch),axis=1)\n            \n#             yield X,y\n            \n","metadata":{"execution":{"iopub.status.busy":"2022-11-06T18:16:09.158471Z","iopub.execute_input":"2022-11-06T18:16:09.159241Z","iopub.status.idle":"2022-11-06T18:16:09.171447Z","shell.execute_reply.started":"2022-11-06T18:16:09.159205Z","shell.execute_reply":"2022-11-06T18:16:09.170514Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Simple collaborative model\ndef collaborative_model(num_users, num_movies, emb_dim):\n    x = tf.keras.Input((2))\n    x_user = tf.keras.layers.Embedding(num_users,emb_dim,\n#                                        embeddings_regularizer=tf.keras.regularizers.L2(1e-4),\n                                       name='users')(x[:,0])\n    x_movie = tf.keras.layers.Embedding(num_movies,emb_dim,\n#                                        embeddings_regularizer=tf.keras.regularizers.L2(1e-4),\n                                       name='movies')(x[:,1])\n    out = tf.linalg.diag_part(tf.linalg.matmul(x_user,x_movie,transpose_b=True))\n    \n    return tf.keras.Model(inputs=x, outputs=out, name=\"colab_model\")","metadata":{"execution":{"iopub.status.busy":"2022-11-06T18:37:32.047091Z","iopub.execute_input":"2022-11-06T18:37:32.047544Z","iopub.status.idle":"2022-11-06T18:37:32.056691Z","shell.execute_reply.started":"2022-11-06T18:37:32.047510Z","shell.execute_reply":"2022-11-06T18:37:32.055333Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def sample_gen():\n#     X = np.random.randint(0,1000,size=(13284744,2))\n#     y = np.random.randint(0,10,size=(13284744))\n    X  = tf.ones((13284744,2))\n    y = tf.ones((13284744))\n    prev_i = 0\n    for i in range(32,X.shape[0],32):\n        yield X[prev_i:i,:], y[prev_i:i]\n\ngen = sample_gen()\n    \n    \n\nmodel = collaborative_model(len(user),len(anime),8)\nmodel.compile(\n    optimizer='adam',\n    loss=tf.keras.losses.MeanSquaredError(),\n    metrics=['mse'])\n\nmodel.fit(gen,\ncallbacks=[tf.keras.callbacks.EarlyStopping(\n    monitor='val_mse',\n    patience=4,\n    restore_best_weights=True\n)])","metadata":{"execution":{"iopub.status.busy":"2022-11-06T19:05:15.967802Z","iopub.execute_input":"2022-11-06T19:05:15.968330Z","iopub.status.idle":"2022-11-06T19:15:15.917162Z","shell.execute_reply.started":"2022-11-06T19:05:15.968286Z","shell.execute_reply":"2022-11-06T19:15:15.914466Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"    981/Unknown - 598s 610ms/step - loss: 0.0241 - mse: 0.1414","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/371387965.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m )])\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"#Create a TF dataset. Due to huge number of users, extremely memory hungry. Need approximation methods\ngen = User_anime_data(files,user_ids=user['serial_index'],anime_ids=anime['serial_index'])\n# dataset = tf.data.Dataset.from_generator(gen.get_gen, \n#           output_signature=(\n#                             tf.TensorSpec(shape=(None,2), dtype=tf.int32),\n#                             tf.TensorSpec(shape=(None), dtype=tf.int32)))\ndataset = gen.get_gen()\n\nmodel = collaborative_model(len(user),len(anime),8)\nmodel.compile(\n    optimizer='adam',\n    loss=tf.keras.losses.MeanSquaredError(),\n    metrics=['mse'])\n\nmodel.fit(dataset,\ncallbacks=[tf.keras.callbacks.EarlyStopping(\n    monitor='val_mse',\n    patience=4,\n    restore_best_weights=True\n)])    \n    \numat = model.get_layer('users').get_weights()[0]\nmmat = model.get_layer('movies').get_weights()[0]\n","metadata":{"execution":{"iopub.status.busy":"2022-11-06T18:16:16.699124Z","iopub.execute_input":"2022-11-06T18:16:16.699490Z","iopub.status.idle":"2022-11-06T18:17:13.743090Z","shell.execute_reply.started":"2022-11-06T18:16:16.699459Z","shell.execute_reply":"2022-11-06T18:17:13.741635Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2022-11-06 18:16:17.688891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-06 18:16:17.693559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-06 18:16:17.694598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-06 18:16:17.696449: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-06 18:16:17.696808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-06 18:16:17.697786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-06 18:16:17.698752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-06 18:16:20.047125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-06 18:16:20.048112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-06 18:16:20.048786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-06 18:16:20.049365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15381 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/myanimelist-dataset/user_anime000000000003.csv\n1819971\n","output_type":"stream"},{"name":"stderr","text":"2022-11-06 18:16:26.957879: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"   1291/Unknown - 36s 27ms/step - loss: 29.8999 - mse: 37.4480","output_type":"stream"},{"name":"stderr","text":"2022-11-06 18:17:13.366963: W tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.37GiB (rounded to 6837305344)requested by op gradient_tape/colab_model/tf.linalg.diag_part/zeros_like\nIf the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \nCurrent allocation summary follows.\nCurrent allocation summary follows.\n2022-11-06 18:17:13.367025: I tensorflow/core/common_runtime/bfc_allocator.cc:1004] BFCAllocator dump for GPU_0_bfc\n2022-11-06 18:17:13.367050: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (256): \tTotal Chunks: 32, Chunks in use: 31. 8.0KiB allocated for chunks. 7.8KiB in use in bin. 156B client-requested in use in bin.\n2022-11-06 18:17:13.367065: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2022-11-06 18:17:13.367084: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1024): \tTotal Chunks: 2, Chunks in use: 1. 3.0KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n2022-11-06 18:17:13.367095: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2022-11-06 18:17:13.367106: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2022-11-06 18:17:13.367116: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2022-11-06 18:17:13.367129: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16384): \tTotal Chunks: 1, Chunks in use: 1. 19.8KiB allocated for chunks. 19.8KiB in use in bin. 19.7KiB client-requested in use in bin.\n2022-11-06 18:17:13.367144: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (32768): \tTotal Chunks: 1, Chunks in use: 1. 37.2KiB allocated for chunks. 37.2KiB in use in bin. 37.1KiB client-requested in use in bin.\n2022-11-06 18:17:13.367240: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2022-11-06 18:17:13.367268: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (131072): \tTotal Chunks: 3, Chunks in use: 3. 484.5KiB allocated for chunks. 484.5KiB in use in bin. 484.5KiB client-requested in use in bin.\n2022-11-06 18:17:13.367280: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (262144): \tTotal Chunks: 4, Chunks in use: 3. 1.52MiB allocated for chunks. 1.26MiB in use in bin. 1.22MiB client-requested in use in bin.\n2022-11-06 18:17:13.367293: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (524288): \tTotal Chunks: 1, Chunks in use: 0. 646.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2022-11-06 18:17:13.367304: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1048576): \tTotal Chunks: 2, Chunks in use: 2. 2.52MiB allocated for chunks. 2.52MiB in use in bin. 2.52MiB client-requested in use in bin.\n2022-11-06 18:17:13.367319: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2022-11-06 18:17:13.367333: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2022-11-06 18:17:13.367349: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2022-11-06 18:17:13.367364: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2022-11-06 18:17:13.367380: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (33554432): \tTotal Chunks: 4, Chunks in use: 3. 166.16MiB allocated for chunks. 103.62MiB in use in bin. 102.84MiB client-requested in use in bin.\n2022-11-06 18:17:13.367394: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 0. 101.64MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2022-11-06 18:17:13.367604: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2022-11-06 18:17:13.367652: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 1. 14.75GiB allocated for chunks. 6.37GiB in use in bin. 6.37GiB client-requested in use in bin.\n2022-11-06 18:17:13.367668: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Bin for 6.37GiB was 256.00MiB, Chunk State: \n2022-11-06 18:17:13.367690: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]   Size: 2.02GiB | Requested Size: 19.7KiB | in_use: 0 | bin_num: 20, prev:   Size: 34.28MiB | Requested Size: 34.28MiB | in_use: 1 | bin_num: -1\n2022-11-06 18:17:13.367709: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]   Size: 6.36GiB | Requested Size: 161.4KiB | in_use: 0 | bin_num: 20, prev:   Size: 6.37GiB | Requested Size: 6.37GiB | in_use: 1 | bin_num: -1, next:   Size: 454.2KiB | Requested Size: 418.1KiB | in_use: 1 | bin_num: -1\n2022-11-06 18:17:13.367720: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 16128933888\n2022-11-06 18:17:13.367751: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18000000 of size 1280 next 1\n2022-11-06 18:17:13.367764: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18000500 of size 256 next 2\n2022-11-06 18:17:13.367774: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18000600 of size 256 next 3\n2022-11-06 18:17:13.367785: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18000700 of size 256 next 4\n2022-11-06 18:17:13.367794: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18000800 of size 256 next 5\n2022-11-06 18:17:13.367804: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18000900 of size 256 next 10\n2022-11-06 18:17:13.367813: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18000a00 of size 256 next 11\n2022-11-06 18:17:13.367822: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18000b00 of size 256 next 12\n2022-11-06 18:17:13.367831: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18000c00 of size 256 next 13\n2022-11-06 18:17:13.367840: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18000d00 of size 256 next 14\n2022-11-06 18:17:13.367849: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18000e00 of size 256 next 15\n2022-11-06 18:17:13.367856: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18000f00 of size 256 next 16\n2022-11-06 18:17:13.367863: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18001000 of size 256 next 17\n2022-11-06 18:17:13.367869: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18001100 of size 256 next 18\n2022-11-06 18:17:13.367876: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18001200 of size 256 next 19\n2022-11-06 18:17:13.367882: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18001300 of size 256 next 65\n2022-11-06 18:17:13.367889: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7f2a18001400 of size 256 next 33\n2022-11-06 18:17:13.367895: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18001500 of size 256 next 46\n2022-11-06 18:17:13.367902: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7f2a18001600 of size 1792 next 8\n2022-11-06 18:17:13.367909: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18001d00 of size 256 next 39\n2022-11-06 18:17:13.367915: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18001e00 of size 256 next 49\n2022-11-06 18:17:13.367922: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18001f00 of size 256 next 66\n2022-11-06 18:17:13.367930: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18002000 of size 256 next 58\n2022-11-06 18:17:13.367936: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18002100 of size 256 next 52\n2022-11-06 18:17:13.367943: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18002200 of size 256 next 53\n2022-11-06 18:17:13.367950: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a18002300 of size 165376 next 37\n2022-11-06 18:17:13.367957: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7f2a1802a900 of size 661504 next 9\n2022-11-06 18:17:13.367964: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a180cc100 of size 36373504 next 54\n2022-11-06 18:17:13.367971: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a1a37c500 of size 428288 next 51\n2022-11-06 18:17:13.367978: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7f2a1a3e4e00 of size 106573568 next 21\n2022-11-06 18:17:13.367984: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a20987d00 of size 256 next 22\n2022-11-06 18:17:13.367991: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a20987e00 of size 256 next 23\n2022-11-06 18:17:13.367997: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a20987f00 of size 256 next 24\n2022-11-06 18:17:13.368004: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a20988000 of size 256 next 25\n2022-11-06 18:17:13.368010: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a20988100 of size 256 next 26\n2022-11-06 18:17:13.368017: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a20988200 of size 256 next 27\n2022-11-06 18:17:13.368023: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a20988300 of size 256 next 28\n2022-11-06 18:17:13.368030: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a20988400 of size 256 next 29\n2022-11-06 18:17:13.368036: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a20988500 of size 256 next 30\n2022-11-06 18:17:13.368045: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2a20988600 of size 6837305344 next 41\n2022-11-06 18:17:13.368051: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7f2bb8218600 of size 6831112192 next 34\n2022-11-06 18:17:13.368058: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2d4f4c0600 of size 465152 next 62\n2022-11-06 18:17:13.368065: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2d4f531f00 of size 36336640 next 48\n2022-11-06 18:17:13.368072: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2d517d9300 of size 165376 next 38\n2022-11-06 18:17:13.368078: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2d51801900 of size 1323008 next 44\n2022-11-06 18:17:13.368085: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2d51944900 of size 38144 next 6\n2022-11-06 18:17:13.368092: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2d5194de00 of size 20224 next 36\n2022-11-06 18:17:13.368098: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7f2d51952d00 of size 272384 next 43\n2022-11-06 18:17:13.368105: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2d51995500 of size 1323008 next 55\n2022-11-06 18:17:13.368111: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2d51ad8500 of size 165376 next 32\n2022-11-06 18:17:13.368118: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7f2d51b00b00 of size 65579264 next 31\n2022-11-06 18:17:13.368124: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2d5598b400 of size 428288 next 20\n2022-11-06 18:17:13.368131: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7f2d559f3d00 of size 35945216 next 59\n2022-11-06 18:17:13.368139: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7f2d57c3b800 of size 2174240768 next 18446744073709551615\n2022-11-06 18:17:13.368145: I tensorflow/core/common_runtime/bfc_allocator.cc:1065]      Summary of in-use Chunks by size: \n2022-11-06 18:17:13.368155: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 31 Chunks of size 256 totalling 7.8KiB\n2022-11-06 18:17:13.368163: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 1280 totalling 1.2KiB\n2022-11-06 18:17:13.368171: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 20224 totalling 19.8KiB\n2022-11-06 18:17:13.368178: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 38144 totalling 37.2KiB\n2022-11-06 18:17:13.368185: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 3 Chunks of size 165376 totalling 484.5KiB\n2022-11-06 18:17:13.368193: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 428288 totalling 836.5KiB\n2022-11-06 18:17:13.368200: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 465152 totalling 454.2KiB\n2022-11-06 18:17:13.368207: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 1323008 totalling 2.52MiB\n2022-11-06 18:17:13.368215: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 35945216 totalling 34.28MiB\n2022-11-06 18:17:13.368222: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 36336640 totalling 34.65MiB\n2022-11-06 18:17:13.368229: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 36373504 totalling 34.69MiB\n2022-11-06 18:17:13.368237: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 6837305344 totalling 6.37GiB\n2022-11-06 18:17:13.368244: I tensorflow/core/common_runtime/bfc_allocator.cc:1072] Sum Total of in-use chunks: 6.47GiB\n2022-11-06 18:17:13.368251: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] total_region_allocated_bytes_: 16128933888 memory_limit_: 16128933888 available bytes: 0 curr_region_allocation_bytes_: 32257867776\n2022-11-06 18:17:13.368262: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] Stats: \nLimit:                     16128933888\nInUse:                      6950492160\nMaxInUse:                  13845218560\nNumAllocs:                      103244\nMaxAllocSize:               6849688576\nReserved:                            0\nPeakReserved:                        0\nLargestFreeBlock:                    0\n\n2022-11-06 18:17:13.368274: W tensorflow/core/common_runtime/bfc_allocator.cc:468] ********************************************_________________________________________**_____________\n2022-11-06 18:17:13.369502: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at constant_op.cc:259 : Resource exhausted: OOM when allocating tensor with shape[41344,41344] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/258032227.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m )])    \n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[41344,41344] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/colab_model/tf.linalg.diag_part/zeros_like (defined at tmp/ipykernel_24/258032227.py:19) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_696]\n\nFunction call stack:\ntrain_function\n"],"ename":"ResourceExhaustedError","evalue":" OOM when allocating tensor with shape[41344,41344] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/colab_model/tf.linalg.diag_part/zeros_like (defined at tmp/ipykernel_24/258032227.py:19) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_696]\n\nFunction call stack:\ntrain_function\n","output_type":"error"}]},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_generator(gen.get_gen, \n          output_signature=(\n                            tf.TensorSpec(shape=(None,2), dtype=tf.int32),\n                            tf.TensorSpec(shape=(None), dtype=tf.int32))\n                                        )\n\nfor i,data in enumerate(dataset):\n#     print(i)\n    if not i%1000:\n        print(i)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-11-05T21:56:33.295875Z","iopub.execute_input":"2022-11-05T21:56:33.296676Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2022-11-05 21:56:33.320705: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n2022-11-05 21:56:33.441474: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/myanimelist-dataset/user_anime000000000015.csv\n1977198\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\n10000\n11000\n12000\n13000\n14000\n15000\n16000\n17000\n18000\n19000\n20000\n21000\n22000\n23000\n24000\n25000\n26000\n","output_type":"stream"}]},{"cell_type":"code","source":"mmat.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-05T21:06:27.050415Z","iopub.execute_input":"2022-11-05T21:06:27.050915Z","iopub.status.idle":"2022-11-05T21:06:27.059998Z","shell.execute_reply.started":"2022-11-05T21:06:27.050880Z","shell.execute_reply":"2022-11-05T21:06:27.058449Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(13379, 8)"},"metadata":{}}]}]}